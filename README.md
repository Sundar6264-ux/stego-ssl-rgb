# üõ∞Ô∏è stego-ssl-rgb ‚Äî Self‚ÄëSupervised Learning with Steganographic Perturbations

A compact research repo exploring **Barlow Twins** self‚Äësupervised learning on **CIFAR‚Äë10**
with **bit‚Äëplane steganography** used as an *intentional perturbation/augmentation*.
Two variants are provided:
- **Green‚Äëchannel bit‚Äëplane** perturbation, and
- **RGB bit‚Äëplane** perturbation.

The pipeline pretrains an encoder using Barlow Twins (two augmented views), then evaluates
representations with a **linear probe** and **fine‚Äëtuning**; utilities generate **t‚ÄëSNE** plots
to visualize separability.

---

## üì¶ What‚Äôs inside

```
stego-ssl-rgb/
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ originalBarlow/                 # Baseline Barlow‚ÄëTwins training + eval
‚îÇ  ‚îÇ  ‚îú‚îÄ Pymain.py                    # SSL pretraining (clean two‚Äëcrop views)
‚îÇ  ‚îÇ  ‚îú‚îÄ lp.py                        # Linear probe (logistic head on frozen encoder)
‚îÇ  ‚îÇ  ‚îú‚îÄ ft_p.py, ft_p_light_adamw.py # Full fine‚Äëtune options
‚îÇ  ‚îÇ  ‚îî‚îÄ eval_bt.py, tst.py           # Misc eval helpers
‚îÇ  ‚îú‚îÄ bit_plane_stego_GreenChannel/   # Stego on Green channel
‚îÇ  ‚îÇ  ‚îú‚îÄ Pretrain_Stego_Model.py      # SSL pretraining (view2 uses bit‚Äëplane stego)
‚îÇ  ‚îÇ  ‚îú‚îÄ linear_probe_vis.py          # Linear probe + visualization
‚îÇ  ‚îÇ  ‚îî‚îÄ Model_Visualize.py           # t‚ÄëSNE / feature visualization
‚îÇ  ‚îî‚îÄ bitpl_stego_RGB/                # Stego across RGB
‚îÇ     ‚îú‚îÄ bitplane_stego_train.py      # SSL pretraining with RGB bit‚Äëplane
‚îÇ     ‚îú‚îÄ ftsne.py, fvis.py            # TSNE and feature viz
‚îÇ     ‚îî‚îÄ st_ev.py                     # Evaluation helpers
‚îú‚îÄ results/                           # Example outputs (accuracy curves, TSNE plots, etc.)
‚îú‚îÄ requirements.txt                   # Python dependencies
‚îî‚îÄ README.md
```

> **Note:** Scripts are intentionally lightweight research drivers. Some flags are hard‚Äëcoded in the files;
> adjust values directly or run with `--help` where available.

---

## üß† Method (high‚Äëlevel)

1. **Two‚Äëview SSL** ‚Äî For each image, create two views:
   - `view1`: standard augmentations (crop, flip, color jitter, etc.).
   - `view2`: same base augs **plus bit‚Äëplane steganography** (flip/embed a chosen bit‚Äëplane).
2. **Barlow Twins** ‚Äî Encode both views, project with an MLP, and minimize the cross‚Äëcorrelation
   objective to align features while reducing redundancy.
3. **Evaluation** ‚Äî Train a **linear probe** on frozen features and optionally **fine‚Äëtune** the full model.
4. **Visualization** ‚Äî Produce **t‚ÄëSNE** scatter plots of embeddings during/after training.

---

## üõ† Requirements

- Python **3.9+** (3.11 is fine)
- A recent **PyTorch** + **torchvision** (CPU or CUDA)
- Other Python libs: `numpy`, `Pillow`, `scikit-learn`, `matplotlib`, `tqdm`

Install everything with:

```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
```

> If your platform needs specific PyTorch wheels (CUDA/cuDNN), prefer the official install selector
> and then `pip install -r requirements.txt --no-deps` to keep your chosen torch/torchvision versions.

---

## üöÄ Quickstart

### 1) SSL pretraining (baseline, clean views)
```bash
python src/originalBarlow/Pymain.py
```
- Downloads **CIFAR‚Äë10** via `torchvision` on first run.
- Outputs checkpoints and logs (see prints).

### 2) SSL pretraining with **Green‚Äëchannel bit‚Äëplane stego**
```bash
python src/bit_plane_stego_GreenChannel/Pretrain_Stego_Model.py
```

### 3) SSL pretraining with **RGB bit‚Äëplane stego**
```bash
python src/bitpl_stego_RGB/bitplane_stego_train.py
```

### 4) Linear probe / fine‚Äëtune / visualization
```bash
# Linear probe (frozen encoder)
python src/originalBarlow/lp.py

# Full fine‚Äëtune
python src/originalBarlow/ft_p.py               # or ft_p_light_adamw.py

# t‚ÄëSNE / feature visualization helpers
python src/bitpl_stego_RGB/ftsne.py
python src/bitpl_stego_RGB/fvis.py
python src/bit_plane_stego_GreenChannel/Model_Visualize.py
```

Artifacts (accuracy curves, TSNE snapshots) will appear under `results/` and alongside the scripts.

---

## üß™ Notes & Tips

- **Reproducibility:** set seeds inside the scripts if you need strict repeatability.
- **Batch size / LR:** adjust to your hardware. Cosine LR schedulers are common in these scripts.
- **Mixed precision:** some files use `torch.amp` (automatic mixed precision) ‚Äî works on CUDA and recent CPUs.
- **NaNs:** if loss becomes NaN, reduce LR, clip gradients, or relax augmentations.
- **CIFAR‚Äë10:** images are 32√ó32; projectors/backbones assume this input size.

---

## üìà Example outputs (see `results/`)

- `full_ft_accuracy.png`, `full_ft_train_loss.png`, `full_ft_tsne.png` (green‚Äëchannel variant)
- `tsne_epoch_*.png` (baseline Barlow Twins)
- `tsne_ssl.png` (RGB stego variant)

---

## üßæ License & Citation

Use any OSI license you prefer (e.g., MIT). If you publish work using this repo, please cite **Barlow Twins** and (optionally) this repository.

**Barlow Twins (ICML 2021) ‚Äî preferred**  
Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). *Barlow Twins: Self-Supervised Learning via Redundancy Reduction.* In **Proceedings of the 38th International Conference on Machine Learning (ICML)**, PMLR 139, 12310‚Äì12320.

**BibTeX (ICML/PMLR)**
```bibtex
@inproceedings{zbontar2021barlow,
  title     = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author    = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{'e}phane},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {12310--12320},
  year      = {2021},
  publisher = {PMLR}
}
```

**Alternative (arXiv)**  
Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). *Barlow Twins: Self-Supervised Learning via Redundancy Reduction.* arXiv:2103.03230.

**BibTeX (arXiv)**
```bibtex
@article{zbontar2021barlow_arxiv,
  title   = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author  = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{'e}phane},
  journal = {arXiv preprint arXiv:2103.03230},
  year    = {2021}
}
```

**(Optional) Cite this repository**
```bibtex
@misc{mamilla2025stego_ssl_rgb,
  title        = {stego-ssl-rgb: Self-Supervised Learning with Steganographic Perturbations},
  author       = {Mamilla, Soma Sundar},
  year         = {2025},
  howpublished = {\url{https://github.com/<your-username>/stego-ssl-rgb}},
  note         = {Version <tag or commit>},
}
```

## üôè Acknowledgments

- PyTorch & TorchVision for models/datasets
- scikit‚Äëlearn for t‚ÄëSNE
- Community work on self‚Äësupervised learning and steganographic data transforms
